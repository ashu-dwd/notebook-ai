```
### Overall Review Summary:
-   **Total Issues Found:** 6
-   **Critical Issues:** 2 (Unhandled synchronous database error, Data Inconsistency/Performance due to non-transactional batch operations)
-   **Code Quality Score:** 6/10
-   **Approval Status:** NEEDS_CHANGES
-   **Key Recommendations:**
    1.  **Ensure Robust Error Handling for Synchronous DB Calls:** The initial synchronous database call for user lookup is outside the `try...catch` block. Any error from this call (e.g., database file not found, malformed query) will crash the application as an unhandled exception. This must be moved inside the `try` block.
    2.  **Optimize Database Writes with Transactions:** While the application uses `better-sqlite3` which is synchronous, performing `N` individual `INSERT` statements for `N` pages is highly inefficient and lacks transactional atomicity. Implement a single batch insert operation within a SQLite transaction to ensure data consistency and significantly improve performance.
    3.  **Improve Modularity (Separation of Concerns):** Extract SQL queries and database operations from the controller logic into dedicated data access layer (e.g., repositories). This enhances maintainability, readability, and testability, and supports a cleaner batch insert implementation.

---

### File-by-File Review

**File:** `server/src/controllers/upload.controller.js`

**Line(s):** 8
-   **Issue Type:** STYLE
    **Description:** The `uuidv4` import is duplicated. It's imported once on line 3 and again on line 8, which is redundant and unnecessary.
    **Line(s) to Fix:** 8
    **Current Code:**
    ```javascript
    import { chunkPdf } from "../services/langchain.service.js";
    import { getEmbeddings } from "../services/llm.service.js";
    import { v4 as uuidv4 } from "uuid";
    import { addVector } from "../services/chromadb.service.js";
    import { logger } from "../utils/logger.js";
    import ApiError from "../utils/ApiError.js";
    import { db, runQuery } from "../database/sqlLite.db.js";
    import { v4 as uuidv4 } from "uuid"; // <--- Duplicated import
    ```
    **Suggested Fix:**
    ```javascript
    import { chunkPdf } from "../services/langchain.service.js";
    import { getEmbeddings } from "../services/llm.service.js";
    import { v4 as uuidv4 } from "uuid"; // Keep this one
    import { addVector } from "../services/chromadb.service.js";
    import { logger } => "../utils/logger.js";
    import ApiError from "../utils/ApiError.js";
    import { db, runQuery } from "../database/sqlLite.db.js";
    // Remove the duplicate import on line 8 entirely.
    ```
    **Priority:** MINOR

**Line(s):** 12-14, 15
-   **Issue Type:** BUG / LOGIC / ERROR_HANDLING
    **Description:** The `runQuery` call to retrieve the user (`const user = runQuery(...)`) is performed *outside* the `try...catch` block. Since `runQuery` is a synchronous function (as implemented in `sqlLite.db.js`) that can throw an `ApiError` or a raw database error on failure, any error occurring during this initial database lookup will be unhandled, potentially crashing the application. While `await` is not needed here because `runQuery` is synchronous, its placement is critical for error resilience.
    **Line(s) to Fix:** 12-14, 15
    **Current Code:**
    ```javascript
    export const handlePdfUpload = async (req, res) => {
      //console.log("üìÇ Uploaded File:", req.file);
      const user = runQuery("SELECT userId FROM userData WHERE email = ? LIMIT 1", [
        req.user.email,
      ]);
      console.log("User found:", user); // This will log the user object or null/undefined
      try {
        logger.info("Uploaded file:", req.file);

        // 1Ô∏è‚É£ Get userId from email

        if (!user) { // This check *is* correct for synchronous runQuery
          throw new ApiError(404, "User not found", [req.user.email]);
        }
        // ... rest of the try block
      } catch (error) { /* ... */ }
    };
    ```
    **Suggested Fix:**
    ```javascript
    export const handlePdfUpload = async (req, res) => {
      // ‚ö†Ô∏è Move initial DB call inside the try...catch block to ensure error handling
      try {
        logger.info("Uploaded file:", req.file);

        // 1Ô∏è‚É£ Get userId from email - This call is synchronous
        const user = runQuery("SELECT userId FROM userData WHERE email = ? LIMIT 1", [
          req.user.email,
        ]);

        // Assuming runQuery for SELECT returns the first row object directly or null/undefined
        // if no row found. For a single row, check for the existence of the user object AND its expected property.
        if (!user || !user.userId) { // Added !user.userId for robustness
          throw new ApiError(404, "User not found", [req.user.email]);
        }
        logger.info("User found:", user); // Replaced console.log with logger.info

        // ... rest of the try block
      } catch (error) {
        // ... (existing error handling, see also the suggested fix for lines 65-67)
      }
    };
    ```
    **Priority:** HIGH

**Line(s):** 15, 44
-   **Issue Type:** STYLE / MAINTAINABILITY
    **Description:** `console.log` statements are used for debugging or informational logging. In a production environment, these should be replaced with the `logger` utility (already imported on line 5) for consistent, configurable, and production-ready logging.
    **Line(s) to Fix:** 15, 44
    **Current Code:**
    ```javascript
    console.log("User found:", user);
    // ...
    console.log("üß© Vector inserted with ID:", vectorResult.id);
    ```
    **Suggested Fix:**
    ```javascript
    logger.info("User found:", user);
    // ...
    logger.info("üß© Vector inserted with ID:", vectorResult.id);
    ```
    **Priority:** MEDIUM

**Line(s):** 49-58 (within `pages.map` loop)
-   **Issue Type:** PERFORMANCE / DATA_CONSISTENCY / LOGIC
    **Description:** Performing an individual database `INSERT` statement for each page inside the `Promise.all` loop (even though `runQuery` is synchronous) leads to several critical issues:
    1.  **Performance Overhead (PERFORMANCE):** Running `N` separate synchronous `INSERT` statements for `N` pages is significantly slower than a single batch insert or a single database transaction. Each `runQuery` call involves preparing and executing a statement, which adds overhead.
    2.  **Data Inconsistency (DATA_CONSISTENCY):** The overall operation (PDF upload, vectorize, and record file metadata) is not atomic. If `addVector` succeeds for a page in ChromaDB but the subsequent `runQuery` for SQLite fails (e.g., due to a database error for *one* of the pages after others succeeded), you will have orphaned vectors in ChromaDB without corresponding entries in your SQLite `files` table. This violates data consistency.
    **Line(s) to Fix:** 49-58
    **Current Code:**
    ```javascript
          const chatId = uuidv4();
          await runQuery( // This call is synchronous, `await` is a no-op but core issue remains
            `
      INSERT INTO files (userId, filePath, vectorId, chatId)\
      VALUES (?, ?, ?, ?)\
    `,\
            [user.userId, req.file.path, vectorResult.id, chatId]\
          );\
    ```
    **Suggested Fix:**
    Collect all data needed for the SQLite inserts *after* the `addVector` calls (which are genuinely asynchronous and require `await`). Then, perform a single batch insert using `better-sqlite3`'s `db.transaction()` method for atomicity and significant performance improvement. This ensures that either all file records are inserted or none are, matching the "all or nothing" principle for the overall PDF processing.

    ```javascript
    // (Assuming previous fixes are applied and user is properly handled)

    // 3Ô∏è‚É£ Process each page and collect data for batch insert
    const fileRecordsToInsert = [];
    const addResults = await Promise.all( // Promise.all is correct for addVector which is async
      pages.map(async (page) => {
        const embedding = await getEmbeddings(page.pageContent);
        const id = uuidv4(); // Vector ID
        const chatId = uuidv4(); // Chat ID for this file/page
        const vectorResult = await addVector({
          id,
          embedding,
          text: page.pageContent,
          metadata: { ...page.metadata, chatId }, // Add chatId to vector metadata if needed
        });

        logger.info("üß© Vector inserted with ID:", vectorResult.id);
        // Collect all necessary data for a single batch SQLite insert later
        fileRecordsToInsert.push([user.userId, req.file.path, vectorResult.id, chatId]);
        return vectorResult.id; // Return vector ID for the overall response
      })
    );

    // 4Ô∏è‚É£ Perform a single batch insert for file metadata using a SQLite transaction
    if (fileRecordsToInsert.length > 0) {
      // Ensure 'db' is correctly imported and available, e.g., from sqlLite.db.js
      // `better-sqlite3` transactions are synchronous and handle BEGIN/COMMIT/ROLLBACK internally.
      try {
        // Define the transaction function
        const insertMany = db.transaction((recs) => {
          const stmt = db.prepare('INSERT INTO files (userId, filePath, vectorId, chatId) VALUES (?, ?, ?, ?)');
          for (const record of recs) {
            stmt.run(...record); // Synchronous run for each record within the transaction
          }
        });
        // Execute the transaction with the collected records
        insertMany(fileRecordsToInsert);
        logger.info(`‚úÖ Successfully inserted ${fileRecordsToInsert.length} file records into SQLite.`);
      } catch (dbError) {
        // better-sqlite3 transactions automatically rollback on error, so no explicit rollback needed here.
        logger.error("‚ùå Error inserting file records into SQLite (rolled back):", dbError);
        // Re-throw with a specific ApiError for consistency
        throw new ApiError(500, "Failed to record file metadata into database due to a transactional error.", [dbError.message]);
      }
    }
    // ... rest of the function (Response)
    ```
    **Priority:** HIGH

**Line(s):** 65-67
-   **Issue Type:** LOGIC / ERROR_HANDLING
    **Description:** The catch block re-throws a new `ApiError` with a generic 500 status. While this is generally good practice for unexpected errors, if the original `error` is already an `ApiError` (e.g., thrown by `ApiError(404, "User not found")` from the refined user check), it's better to re-throw the original `ApiError` to preserve its specific status code and message. Otherwise, a specific client-facing error (like 404) could be masked as a generic 500.
    **Line(s) to Fix:** 65-67
    **Current Code:**
    ```javascript
    } catch (error) {
      logger.error("‚ùå Error in PDF upload handling:", error);
      throw new ApiError(500, "Failed to process PDF upload", [error.message]);
    }
    ```
    **Suggested Fix:**
    ```javascript
    } catch (error) {
      logger.error("‚ùå Error in PDF upload handling:", error);
      // If the error is already an ApiError, re-throw it to preserve its type and status
      if (error instanceof ApiError) {
        throw error;
      }
      // Otherwise, wrap generic errors in a 500 ApiError
      throw new ApiError(500, "Failed to process PDF upload", [error.message]);
    }
    ```
    **Priority:** MEDIUM

**Line(s):** 12-14, 50-58
-   **Issue Type:** MAINTAINABILITY / DESIGN
    **Description:** SQL query strings are hardcoded directly within the controller logic. While the parameters are correctly parameterized (preventing direct SQL injection here), separating SQL queries into a dedicated data access layer (e.g., a repository or DAO) significantly improves maintainability, readability, and testability. It allows the controller to focus solely on request handling and business logic, delegating data persistence concerns to specialized modules. This also makes the batch insert solution cleaner and more manageable.
    **Line(s) to Fix:** 12-14, 50-58
    **Current Code:**
    ```javascript
    const user = runQuery("SELECT userId FROM userData WHERE email = ? LIMIT 1", [
      req.user.email,
    ]);
    // ...
          await runQuery(
            `
      INSERT INTO files (userId, filePath, vectorId, chatId)
      VALUES (?, ?, ?, ?)
    `,
            [user.userId, req.file.path, vectorResult.id, chatId]
          );
    ```
    **Suggested Fix:**
    Create dedicated repository modules (e.g., `user.repository.js` and `file.repository.js`) to encapsulate database operations. Note that these repository functions will be synchronous, as `better-sqlite3` and the existing `runQuery` are synchronous.

    `server/src/repositories/user.repository.js`:
    ```javascript
    import { runQuery } from "../database/sqlLite.db.js"; // Adjust path as necessary

    export const getUserByEmail = (email) => { // Not async as runQuery is synchronous
      // `runQuery` is expected to return the first row directly or null/undefined
      const result = runQuery("SELECT userId FROM userData WHERE email = ? LIMIT 1", [email]);
      return result;
    };
    ```

    `server/src/repositories/file.repository.js` (incorporates the batch insert logic):
    ```javascript
    import { db } from "../database/sqlLite.db.js"; // Only need `db` for transactions

    // If you ever need to insert a single record outside a batch (synchronous):
    export const insertFileRecord = (userId, filePath, vectorId, chatId) => {
      const stmt = db.prepare('INSERT INTO files (userId, filePath, vectorId, chatId) VALUES (?, ?, ?, ?)');
      return stmt.run(userId, filePath, vectorId, chatId);
    };

    export const batchInsertFileRecords = (records) => {
      if (!db || typeof db.transaction !== 'function') {
        throw new Error("Database connection object (db) is not properly initialized or does not support transactions.");
      }

      // better-sqlite3's transaction method returns a function that executes the transaction
      const insertMany = db.transaction((recs) => {
        const stmt = db.prepare('INSERT INTO files (userId, filePath, vectorId, chatId) VALUES (?, ?, ?, ?)');
        for (const record of recs) {
          stmt.run(...record); // Synchronous run
        }
      });

      // Execute the transaction. This will automatically handle BEGIN/COMMIT/ROLLBACK.
      insertMany(records);
      return true; // Indicate success if no error thrown
    };
    ```

    Then, refactor `upload.controller.js` to use these repositories:
    ```javascript
    import { chunkPdf } from "../services/langchain.service.js";
    import { getEmbeddings } from "../services/llm.service.js";
    import { v4 as uuidv4 } from "uuid";
    import { addVector } from "../services/chromadb.service.js";
    import { logger } from "../utils/logger.js";
    import ApiError from "../utils/ApiError.js";
    import { db } from "../database/sqlLite.db.js"; // Keep `db` if batchInsertFileRecords uses it directly
    import { getUserByEmail } from "../repositories/user.repository.js"; // New import
    import { batchInsertFileRecords } from "../repositories/file.repository.js"; // New import


    export const handlePdfUpload = async (req, res) => {
      try {
        logger.info("Uploaded file:", req.file);

        // 1Ô∏è‚É£ Get userId from email using repository (synchronous call)
        const user = getUserByEmail(req.user.email); // No await needed

        if (!user || !user.userId) {
          throw new ApiError(404, "User not found", [req.user.email]);
        }
        logger.info("User found:", user);

        // 2Ô∏è‚É£ Chunk the PDF into pages
        const pages = await chunkPdf(req.file.path);

        // 3Ô∏è‚É£ Process each page and collect data for batch insert
        const fileRecordsToInsert = [];
        const addResults = await Promise.all(
          pages.map(async (page) => {
            const embedding = await getEmbeddings(page.pageContent);
            const id = uuidv4(); // Vector ID
            const chatId = uuidv4(); // Chat ID for this file/page
            const vectorResult = await addVector({
              id,
              embedding,
              text: page.pageContent,
              metadata: { ...page.metadata, chatId }, // Add chatId to vector metadata if needed
            });

            logger.info("üß© Vector inserted with ID:", vectorResult.id);
            fileRecordsToInsert.push([user.userId, req.file.path, vectorResult.id, chatId]);
            return vectorResult.id;
          })
        );

        // 4Ô∏è‚É£ Perform a single batch insert for file metadata using repository (synchronous call)
        if (fileRecordsToInsert.length > 0) {
          batchInsertFileRecords(fileRecordsToInsert); // No await needed
        }

        // 5Ô∏è‚É£ Response
        logger.info(`Added ${addResults.length} page vectors successfully.`);
        return res.json({
          success: true,
          count: addResults.length,
          message: "PDF uploaded and processed successfully",
        });
      } catch (error) {
        logger.error("‚ùå Error in PDF upload handling:", error);
        if (error instanceof ApiError) {
          throw error;
        }
        throw new ApiError(500, "Failed to process PDF upload", [error.message]);
      }
    };
    ```
    **Priority:** MEDIUM
```